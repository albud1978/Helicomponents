#!/usr/bin/env python3
"""
Excel Quality Analyzer - –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö Excel
–ü—Ä–æ–µ–∫—Ç: Helicopter Component Lifecycle Prediction

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö Excel —Ñ–∞–π–ª–æ–≤
–ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –≤ —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤–µ—Ä—Ç–æ–ª–µ—Ç–æ–≤.

–ê–≤—Ç–æ—Ä: AI Assistant
–í–µ—Ä—Å–∏—è: 1.0
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime, timedelta
import sys
import argparse
import json
import warnings
warnings.filterwarnings('ignore')

class ExcelQualityAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö Excel —Ñ–∞–π–ª–æ–≤
    
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    - –í–∞–ª–∏–¥–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª
    - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    - –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
    """
    
    def __init__(self, excel_path: str, log_level: str = 'INFO'):
        self.excel_path = Path(excel_path)
        self.logger = self._setup_logging(log_level)
        self.df = None
        self.analysis_results = {
            'file_info': {},
            'structure_analysis': {},
            'data_quality': {},
            'business_rules': {},
            'statistical_analysis': {},
            'anomalies': {},
            'recommendations': [],
            'quality_score': 0
        }
        
    def _setup_logging(self, log_level: str) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logger = logging.getLogger('ExcelQualityAnalyzer')
        logger.setLevel(getattr(logging, log_level.upper()))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def load_excel(self, sheet_name: str = None) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ Excel —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        
        Args:
            sheet_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç)
            
        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏
        """
        try:
            self.logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ Excel —Ñ–∞–π–ª–∞: {self.excel_path}")
            
            if not self.excel_path.exists():
                self.logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.excel_path}")
                return False
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = self.excel_path.stat().st_size
            self.analysis_results['file_info'] = {
                'path': str(self.excel_path),
                'size_bytes': file_size,
                'size_mb': round(file_size / (1024 * 1024), 2),
                'modified': datetime.fromtimestamp(self.excel_path.stat().st_mtime).isoformat()
            }
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å Arrow backend –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
            try:
                self.df = pd.read_excel(
                    self.excel_path,
                    sheet_name=sheet_name,
                    engine='openpyxl',
                    dtype_backend="pyarrow"
                )
                self.logger.info("‚ö° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω Arrow backend –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
            except Exception as e:
                self.logger.warning(f"Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —á—Ç–µ–Ω–∏—é: {e}")
                self.df = pd.read_excel(self.excel_path, sheet_name=sheet_name)
            
            self.logger.info(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.df)} —Å—Ç—Ä–æ–∫, {len(self.df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Excel: {e}")
            return False
    
    def analyze_structure(self):
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö...")
        
        if self.df is None:
            return
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        structure = {
            'rows': len(self.df),
            'columns': len(self.df.columns),
            'memory_usage_mb': round(self.df.memory_usage(deep=True).sum() / (1024 * 1024), 2),
            'data_types': {},
            'missing_data': {},
            'duplicate_analysis': {}
        }
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        for col in self.df.columns:
            dtype = str(self.df[col].dtype)
            null_count = self.df[col].isnull().sum()
            null_percentage = (null_count / len(self.df)) * 100
            
            structure['data_types'][col] = {
                'dtype': dtype,
                'null_count': null_count,
                'null_percentage': round(null_percentage, 2),
                'unique_values': self.df[col].nunique(),
                'memory_usage_bytes': self.df[col].memory_usage(deep=True)
            }
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if null_count > 0:
                structure['missing_data'][col] = {
                    'count': null_count,
                    'percentage': round(null_percentage, 2),
                    'pattern': self._analyze_missing_pattern(col)
                }
        
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        total_duplicates = self.df.duplicated().sum()
        structure['duplicate_analysis'] = {
            'total_duplicates': total_duplicates,
            'duplicate_percentage': round((total_duplicates / len(self.df)) * 100, 2)
        }
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–æ–ª—è–º
        key_fields = ['partno', 'serialno']
        available_keys = [field for field in key_fields if field in self.df.columns]
        
        if len(available_keys) >= 2:
            key_duplicates = self.df.duplicated(subset=available_keys).sum()
            structure['duplicate_analysis']['key_duplicates'] = {
                'fields': available_keys,
                'count': key_duplicates,
                'percentage': round((key_duplicates / len(self.df)) * 100, 2)
            }
        
        self.analysis_results['structure_analysis'] = structure
        self.logger.info(f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
    
    def _analyze_missing_pattern(self, column: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        missing_mask = self.df[column].isnull()
        
        if missing_mask.sum() == 0:
            return "no_missing"
        elif missing_mask.sum() == len(self.df):
            return "all_missing"
        elif missing_mask.iloc[:100].all():
            return "missing_at_start"
        elif missing_mask.iloc[-100:].all():
            return "missing_at_end"
        else:
            return "scattered"
    
    def analyze_data_quality(self):
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        if self.df is None:
            return
        
        quality = {
            'critical_fields': {},
            'date_fields': {},
            'numeric_fields': {},
            'text_fields': {},
            'quality_issues': []
        }
        
        # –ê–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π
        critical_fields = ['partno', 'serialno']
        for field in critical_fields:
            if field in self.df.columns:
                field_analysis = self._analyze_critical_field(field)
                quality['critical_fields'][field] = field_analysis
                
                if field_analysis['issues']:
                    quality['quality_issues'].extend(field_analysis['issues'])
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–µ–π –¥–∞—Ç
        date_fields = ['mfg_date', 'removal_date', 'target_date', 'oh_at_date', 'repair_date']
        for field in date_fields:
            if field in self.df.columns:
                date_analysis = self._analyze_date_field(field)
                quality['date_fields'][field] = date_analysis
                
                if date_analysis['issues']:
                    quality['quality_issues'].extend(date_analysis['issues'])
        
        # –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π
        numeric_fields = ['oh', 'oh_threshold', 'll', 'sne', 'ppr', 'daily_flight_hours']
        for field in numeric_fields:
            if field in self.df.columns:
                numeric_analysis = self._analyze_numeric_field(field)
                quality['numeric_fields'][field] = numeric_analysis
                
                if numeric_analysis['issues']:
                    quality['quality_issues'].extend(numeric_analysis['issues'])
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π
        text_fields = ['condition', 'location', 'ac_typ', 'owner']
        for field in text_fields:
            if field in self.df.columns:
                text_analysis = self._analyze_text_field(field)
                quality['text_fields'][field] = text_analysis
        
        self.analysis_results['data_quality'] = quality
        self.logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(quality['quality_issues'])} –ø—Ä–æ–±–ª–µ–º")
    
    def _analyze_critical_field(self, field: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ –ø–æ–ª—è"""
        series = self.df[field]
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        null_count = series.isnull().sum()
        empty_count = (series.astype(str).str.strip() == '').sum()
        
        if null_count > 0:
            issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: {field} –∏–º–µ–µ—Ç {null_count} –ø—É—Å—Ç—ã—Ö (NULL) –∑–Ω–∞—á–µ–Ω–∏–π")
        
        if empty_count > 0:
            issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: {field} –∏–º–µ–µ—Ç {empty_count} –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫")
        
        # –ê–Ω–∞–ª–∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        unique_count = series.nunique()
        duplicate_count = len(series) - unique_count
        
        return {
            'total_values': len(series),
            'null_count': null_count,
            'empty_count': empty_count,
            'unique_count': unique_count,
            'duplicate_count': duplicate_count,
            'sample_values': series.dropna().astype(str).head(5).tolist(),
            'issues': issues
        }
    
    def _analyze_date_field(self, field: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–ª—è –¥–∞—Ç—ã"""
        series = self.df[field]
        issues = []
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –¥–∞—Ç—ã
        try:
            dates = pd.to_datetime(series, errors='coerce', dayfirst=True)
            valid_dates = dates.dropna()
            invalid_count = len(series) - len(valid_dates)
            
            analysis = {
                'total_values': len(series),
                'valid_dates': len(valid_dates),
                'invalid_dates': invalid_count,
                'issues': issues
            }
            
            if len(valid_dates) > 0:
                min_date = valid_dates.min()
                max_date = valid_dates.max()
                current_date = pd.Timestamp.now()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑—É–º–Ω–æ—Å—Ç–∏ –¥–∞—Ç
                future_dates = (valid_dates > current_date).sum()
                old_dates = (valid_dates < pd.Timestamp('1980-01-01')).sum()
                
                analysis.update({
                    'min_date': min_date.isoformat(),
                    'max_date': max_date.isoformat(),
                    'date_range_days': (max_date - min_date).days,
                    'future_dates': future_dates,
                    'old_dates': old_dates
                })
                
                if future_dates > 0:
                    issues.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {future_dates} –¥–∞—Ç –≤ –±—É–¥—É—â–µ–º")
                
                if old_dates > 0:
                    issues.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {old_dates} –¥–∞—Ç –¥–æ 1980 –≥–æ–¥–∞")
            
            if invalid_count > 0:
                invalid_percentage = (invalid_count / len(series)) * 100
                if invalid_percentage > 20:
                    issues.append(f"–ü–†–û–ë–õ–ï–ú–ê: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {invalid_count} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞—Ç ({invalid_percentage:.1f}%)")
                    
        except Exception as e:
            issues.append(f"–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–ª–µ –¥–∞—Ç {field}: {e}")
            analysis = {'total_values': len(series), 'issues': issues}
        
        return analysis
    
    def _analyze_numeric_field(self, field: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø–æ–ª—è"""
        series = self.df[field]
        issues = []
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–∞
        try:
            numeric_series = pd.to_numeric(series, errors='coerce')
            valid_numbers = numeric_series.dropna()
            invalid_count = len(series) - len(valid_numbers)
            
            analysis = {
                'total_values': len(series),
                'valid_numbers': len(valid_numbers),
                'invalid_numbers': invalid_count,
                'issues': issues
            }
            
            if len(valid_numbers) > 0:
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                stats = {
                    'min': valid_numbers.min(),
                    'max': valid_numbers.max(),
                    'mean': valid_numbers.mean(),
                    'median': valid_numbers.median(),
                    'std': valid_numbers.std(),
                    'zeros': (valid_numbers == 0).sum(),
                    'negatives': (valid_numbers < 0).sum()
                }
                
                analysis['statistics'] = stats
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
                if stats['negatives'] > 0:
                    if field in ['sne', 'ppr', 'oh', 'll']:  # –ü–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏
                        issues.append(f"–ö–†–ò–¢–ò–ß–ù–û: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {stats['negatives']} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
                    else:
                        issues.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {stats['negatives']} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
                
                # –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ (–∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ 1.5 * IQR)
                Q1 = valid_numbers.quantile(0.25)
                Q3 = valid_numbers.quantile(0.75)
                IQR = Q3 - Q1
                
                outliers = valid_numbers[(valid_numbers < (Q1 - 1.5 * IQR)) | 
                                       (valid_numbers > (Q3 + 1.5 * IQR))]
                
                if len(outliers) > 0:
                    outlier_percentage = (len(outliers) / len(valid_numbers)) * 100
                    analysis['outliers'] = {
                        'count': len(outliers),
                        'percentage': round(outlier_percentage, 2),
                        'examples': outliers.head(3).tolist()
                    }
                    
                    if outlier_percentage > 5:
                        issues.append(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {len(outliers)} –≤—ã–±—Ä–æ—Å–æ–≤ ({outlier_percentage:.1f}%)")
            
            if invalid_count > 0:
                invalid_percentage = (invalid_count / len(series)) * 100
                if invalid_percentage > 10:
                    issues.append(f"–ü–†–û–ë–õ–ï–ú–ê: {field} —Å–æ–¥–µ—Ä–∂–∏—Ç {invalid_count} –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ({invalid_percentage:.1f}%)")
                    
        except Exception as e:
            issues.append(f"–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —á–∏—Å–ª–æ–≤–æ–µ –ø–æ–ª–µ {field}: {e}")
            analysis = {'total_values': len(series), 'issues': issues}
        
        return analysis
    
    def _analyze_text_field(self, field: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è"""
        series = self.df[field].astype(str)
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        non_null_series = series[series != 'nan']
        
        analysis = {
            'total_values': len(series),
            'non_null_values': len(non_null_series),
            'unique_values': non_null_series.nunique(),
            'max_length': non_null_series.str.len().max() if len(non_null_series) > 0 else 0,
            'min_length': non_null_series.str.len().min() if len(non_null_series) > 0 else 0,
            'avg_length': round(non_null_series.str.len().mean(), 2) if len(non_null_series) > 0 else 0,
            'empty_strings': (non_null_series.str.strip() == '').sum(),
            'value_distribution': non_null_series.value_counts().head(10).to_dict()
        }
        
        return analysis
    
    def analyze_business_rules(self):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞–º"""
        self.logger.info("üìã –ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª...")
        
        if self.df is None:
            return
        
        business_rules = {
            'rule_violations': [],
            'consistency_checks': {}
        }
        
        # –ü—Ä–∞–≤–∏–ª–æ 1: –î–∞—Ç–∞ —Å–Ω—è—Ç–∏—è –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–Ω—å—à–µ –¥–∞—Ç—ã –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è
        if 'mfg_date' in self.df.columns and 'removal_date' in self.df.columns:
            try:
                mfg_dates = pd.to_datetime(self.df['mfg_date'], errors='coerce')
                removal_dates = pd.to_datetime(self.df['removal_date'], errors='coerce')
                
                invalid_sequence = ((removal_dates < mfg_dates) & 
                                  mfg_dates.notna() & removal_dates.notna()).sum()
                
                business_rules['consistency_checks']['date_sequence'] = {
                    'description': '–î–∞—Ç–∞ —Å–Ω—è—Ç–∏—è –ø–æ—Å–ª–µ –¥–∞—Ç—ã –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è',
                    'violations': invalid_sequence,
                    'passed': invalid_sequence == 0
                }
                
                if invalid_sequence > 0:
                    business_rules['rule_violations'].append(
                        f"–ù–ê–†–£–®–ï–ù–ò–ï –õ–û–ì–ò–ö–ò: {invalid_sequence} —Å–ª—É—á–∞–µ–≤ —Å–Ω—è—Ç–∏—è —Ä–∞–Ω—å—à–µ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è"
                    )
                    
            except Exception as e:
                business_rules['rule_violations'].append(f"–û–®–ò–ë–ö–ê –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞—Ç: {e}")
        
        # –ü—Ä–∞–≤–∏–ª–æ 2: SNE –∏ PPR –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏
        for field in ['sne', 'ppr']:
            if field in self.df.columns:
                try:
                    numeric_series = pd.to_numeric(self.df[field], errors='coerce')
                    negative_count = (numeric_series < 0).sum()
                    
                    business_rules['consistency_checks'][f'{field}_positive'] = {
                        'description': f'{field.upper()} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º',
                        'violations': negative_count,
                        'passed': negative_count == 0
                    }
                    
                    if negative_count > 0:
                        business_rules['rule_violations'].append(
                            f"–ù–ê–†–£–®–ï–ù–ò–ï –ü–†–ê–í–ò–õ–ê: {field.upper()} —Å–æ–¥–µ—Ä–∂–∏—Ç {negative_count} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"
                        )
                        
                except Exception as e:
                    business_rules['rule_violations'].append(f"–û–®–ò–ë–ö–ê –ø—Ä–æ–≤–µ—Ä–∫–∏ {field}: {e}")
        
        # –ü—Ä–∞–≤–∏–ª–æ 3: –°–µ—Ä–∏–π–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ä—Ç–Ω–æ–º–µ—Ä–∞
        if 'partno' in self.df.columns and 'serialno' in self.df.columns:
            try:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–∞—Ä—Ç–Ω–æ–º–µ—Ä—É –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–µ—Ä–∏–π–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
                duplicates_within_partno = []
                
                for partno, group in self.df.groupby('partno'):
                    dup_serials = group[group.duplicated(subset=['serialno'], keep=False)]
                    if not dup_serials.empty:
                        duplicates_within_partno.append({
                            'partno': partno,
                            'duplicate_serials': dup_serials['serialno'].tolist()
                        })
                
                business_rules['consistency_checks']['serial_uniqueness'] = {
                    'description': '–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å–µ—Ä–∏–π–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ä—Ç–Ω–æ–º–µ—Ä–∞',
                    'violations': len(duplicates_within_partno),
                    'passed': len(duplicates_within_partno) == 0,
                    'details': duplicates_within_partno[:5]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                }
                
                if duplicates_within_partno:
                    business_rules['rule_violations'].append(
                        f"–ù–ê–†–£–®–ï–ù–ò–ï –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–ò: {len(duplicates_within_partno)} –ø–∞—Ä—Ç–Ω–æ–º–µ—Ä–æ–≤ —Å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–µ—Ä–∏–π–Ω—ã–º–∏ –Ω–æ–º–µ—Ä–∞–º–∏"
                    )
                    
            except Exception as e:
                business_rules['rule_violations'].append(f"–û–®–ò–ë–ö–ê –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        
        self.analysis_results['business_rules'] = business_rules
        self.logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(business_rules['rule_violations'])} –Ω–∞—Ä—É—à–µ–Ω–∏–π")
    
    def calculate_quality_score(self):
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üéØ –†–∞—Å—á–µ—Ç –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞...")
        
        if self.df is None:
            self.analysis_results['quality_score'] = 0
            return
        
        score = 100.0  # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ 100%
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        structure = self.analysis_results.get('structure_analysis', {})
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª—è—Ö
        data_quality = self.analysis_results.get('data_quality', {})
        critical_fields = data_quality.get('critical_fields', {})
        
        for field, analysis in critical_fields.items():
            null_percentage = (analysis['null_count'] / analysis['total_values']) * 100
            empty_percentage = (analysis['empty_count'] / analysis['total_values']) * 100
            
            if null_percentage > 0:
                score -= min(20, null_percentage * 2)  # –î–æ 20 –±–∞–ª–ª–æ–≤ –∑–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if empty_percentage > 0:
                score -= min(15, empty_percentage * 1.5)  # –î–æ 15 –±–∞–ª–ª–æ–≤ –∑–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        duplicate_percentage = structure.get('duplicate_analysis', {}).get('duplicate_percentage', 0)
        if duplicate_percentage > 0:
            score -= min(15, duplicate_percentage * 3)  # –î–æ 15 –±–∞–ª–ª–æ–≤ –∑–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        quality_issues = data_quality.get('quality_issues', [])
        critical_issues = [issue for issue in quality_issues if '–ö–†–ò–¢–ò–ß–ù–û' in issue]
        warning_issues = [issue for issue in quality_issues if '–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï' in issue]
        
        score -= len(critical_issues) * 5  # –ü–æ 5 –±–∞–ª–ª–æ–≤ –∑–∞ –∫—Ä–∏—Ç–∏—á–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É
        score -= len(warning_issues) * 2   # –ü–æ 2 –±–∞–ª–ª–∞ –∑–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª
        business_rules = self.analysis_results.get('business_rules', {})
        rule_violations = business_rules.get('rule_violations', [])
        score -= len(rule_violations) * 10  # –ü–æ 10 –±–∞–ª–ª–æ–≤ –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω 0-100
        score = max(0, min(100, score))
        
        self.analysis_results['quality_score'] = round(score, 1)
        self.logger.info(f"üéØ –ë–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞: {score:.1f}/100")
    
    def generate_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üí° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        structure = self.analysis_results.get('structure_analysis', {})
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
        dup_percentage = structure.get('duplicate_analysis', {}).get('duplicate_percentage', 0)
        if dup_percentage > 0:
            recommendations.append({
                'category': '–î—É–±–ª–∏–∫–∞—Ç—ã',
                'priority': 'HIGH' if dup_percentage > 5 else 'MEDIUM',
                'issue': f"–ù–∞–π–¥–µ–Ω–æ {dup_percentage:.1f}% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤",
                'recommendation': "–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∏–ª–∏ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è",
                'action': "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ df.drop_duplicates() –∏–ª–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–∏—á–∏–Ω—ã –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        data_quality = self.analysis_results.get('data_quality', {})
        
        # –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–æ–ª—è
        critical_fields = data_quality.get('critical_fields', {})
        for field, analysis in critical_fields.items():
            if analysis['null_count'] > 0 or analysis['empty_count'] > 0:
                recommendations.append({
                    'category': '–ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–æ–ª—è',
                    'priority': 'HIGH',
                    'issue': f"–ü–æ–ª–µ {field} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                    'recommendation': f"–ó–∞–ø–æ–ª–Ω–∏—Ç—å –∏–ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º–∏ {field}",
                    'action': f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—è {field}"
                })
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞—Ç–∞–º–∏
        date_fields = data_quality.get('date_fields', {})
        for field, analysis in date_fields.items():
            if analysis.get('invalid_dates', 0) > 0:
                recommendations.append({
                    'category': '–î–∞—Ç—ã',
                    'priority': 'MEDIUM',
                    'issue': f"–ü–æ–ª–µ {field} —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞—Ç—ã",
                    'recommendation': "–ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç –∏–ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏",
                    'action': f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pd.to_datetime() —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è {field}"
                })
        
        # –ü—Ä–æ–±–ª–µ–º—ã —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –ø–æ–ª—è–º–∏
        numeric_fields = data_quality.get('numeric_fields', {})
        for field, analysis in numeric_fields.items():
            if analysis.get('invalid_numbers', 0) > 0:
                recommendations.append({
                    'category': '–ß–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è',
                    'priority': 'MEDIUM',
                    'issue': f"–ü–æ–ª–µ {field} —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                    'recommendation': "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –∏–ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                    'action': f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pd.to_numeric() —Å errors='coerce' –¥–ª—è {field}"
                })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª
        business_rules = self.analysis_results.get('business_rules', {})
        rule_violations = business_rules.get('rule_violations', [])
        
        for violation in rule_violations:
            recommendations.append({
                'category': '–ë–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞',
                'priority': 'HIGH',
                'issue': violation,
                'recommendation': "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–æ–π",
                'action': "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"
            })
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        quality_score = self.analysis_results.get('quality_score', 0)
        
        if quality_score < 70:
            recommendations.append({
                'category': '–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ',
                'priority': 'HIGH',
                'issue': f"–ù–∏–∑–∫–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {quality_score}/100",
                'recommendation': "–ü—Ä–æ–≤–µ—Å—Ç–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π",
                'action': "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∞–Ω–∞–ª–∏–∑"
            })
        elif quality_score < 85:
            recommendations.append({
                'category': '–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ',
                'priority': 'MEDIUM',
                'issue': f"–°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {quality_score}/100",
                'recommendation': "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö",
                'action': "–ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"
            })
        
        self.analysis_results['recommendations'] = recommendations
        self.logger.info(f"üí° –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    
    def run_full_analysis(self, sheet_name: str = None) -> dict:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            sheet_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ Excel
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        self.logger.info("üöÄ === –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ö–ê–ß–ï–°–¢–í–ê EXCEL ===")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if not self.load_excel(sheet_name):
            return self.analysis_results
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∏–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        self.analyze_structure()
        self.analyze_data_quality()
        self.analyze_business_rules()
        self.calculate_quality_score()
        self.generate_recommendations()
        
        self.logger.info("üéâ === –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù ===")
        return self.analysis_results
    
    def save_report(self, output_path: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ JSON —Ñ–∞–π–ª"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"excel_quality_report_{timestamp}.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
            return None
    
    def print_summary(self):
        """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞"""
        print("\n" + "="*80)
        print("üìä –†–ï–ó–Æ–ú–ï –ê–ù–ê–õ–ò–ó–ê –ö–ê–ß–ï–°–¢–í–ê EXCEL –î–ê–ù–ù–´–•")
        print("="*80)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        file_info = self.analysis_results.get('file_info', {})
        structure = self.analysis_results.get('structure_analysis', {})
        
        print(f"üìÅ –§–∞–π–ª: {file_info.get('path', 'N/A')}")
        print(f"üìè –†–∞–∑–º–µ—Ä: {file_info.get('size_mb', 0)} –ú–ë")
        print(f"üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {structure.get('rows', 0):,} —Å—Ç—Ä–æ–∫ √ó {structure.get('columns', 0)} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        # –ë–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = self.analysis_results.get('quality_score', 0)
        if quality_score >= 85:
            status = "üü¢ –û–¢–õ–ò–ß–ù–û–ï"
        elif quality_score >= 70:
            status = "üü° –•–û–†–û–®–ï–ï"
        elif quality_score >= 50:
            status = "üü† –°–†–ï–î–ù–ï–ï"
        else:
            status = "üî¥ –ù–ò–ó–ö–û–ï"
        
        print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {quality_score}/100 - {status}")
        
        # –ü—Ä–æ–±–ª–µ–º—ã
        data_quality = self.analysis_results.get('data_quality', {})
        business_rules = self.analysis_results.get('business_rules', {})
        
        quality_issues_count = len(data_quality.get('quality_issues', []))
        rule_violations_count = len(business_rules.get('rule_violations', []))
        
        print(f"‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞: {quality_issues_count}")
        print(f"üö® –ù–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª: {rule_violations_count}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = self.analysis_results.get('recommendations', [])
        high_priority = len([r for r in recommendations if r.get('priority') == 'HIGH'])
        medium_priority = len([r for r in recommendations if r.get('priority') == 'MEDIUM'])
        
        print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {high_priority} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö, {medium_priority} —Å—Ä–µ–¥–Ω–∏—Ö")
        
        # –¢–æ–ø –ø—Ä–æ–±–ª–µ–º—ã
        if quality_issues_count > 0:
            print(f"\nüîç –¢–û–ü –ü–†–û–ë–õ–ï–ú–´:")
            for i, issue in enumerate(data_quality.get('quality_issues', [])[:3], 1):
                print(f"   {i}. {issue}")
        
        if rule_violations_count > 0:
            print(f"\nüö® –ù–ê–†–£–®–ï–ù–ò–Ø –ü–†–ê–í–ò–õ:")
            for i, violation in enumerate(business_rules.get('rule_violations', [])[:3], 1):
                print(f"   {i}. {violation}")
        
        print("="*80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ Excel –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('excel_file', help='–ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É')
    parser.add_argument('--sheet', help='–ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç)')
    parser.add_argument('--output', help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ JSON')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è')
    
    args = parser.parse_args()
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = ExcelQualityAnalyzer(args.excel_file, args.log_level)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        results = analyzer.run_full_analysis(args.sheet)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—é–º–µ
        analyzer.print_summary()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if args.output:
            analyzer.save_report(args.output)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = results.get('quality_score', 0)
        
        if quality_score >= 85:
            sys.exit(0)  # –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        elif quality_score >= 70:
            sys.exit(1)  # –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –µ—Å—Ç—å –∑–∞–º–µ—á–∞–Ω–∏—è
        elif quality_score >= 50:
            sys.exit(2)  # –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω—É–∂–Ω—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        else:
            sys.exit(3)  # –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
        sys.exit(4)


if __name__ == '__main__':
    main() 